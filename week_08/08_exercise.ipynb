{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this excerises we are going to demonsrate how to extract raw sources of data from the internet. This will be split into two sections. First we will consider a simple case where data is already structured and stored in a file, we will show how to use Python code to download and manipulate this data directly. The second case is dealing with automated extraction of unstructured data from a web page. This process is called scraping.\n",
    "\n",
    "1. First we need to install the required packages.\n",
    "- requests : This package allows python code to access a web page via its hyperlink.\n",
    "- BeautifulSoup : This package can parse a web page as HTML and convert it into a python object with a nested structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install requests\n",
    "!pip install beautifulsoup4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First lets look at the requests package and its functions. The most useful function of the requests package is 'get'. It performs a HTTP request to a specific webpage. \n",
    "\n",
    "**HTTP Request**\n",
    "\n",
    "HTTP stands for HyperText Transfer Protocol and is used for transferring data over the internet. A HTTP request is essentially a way for a client (like your web browser or a Python script) to communicate with a server (where a website's data is stored). When you type a URL into your web browser, it is actually sending a HTTP request to a server.\n",
    "\n",
    "These requests can be of different types, called _methods_, examples of which include GET, POST, PUT, DELETE, etc. These methods tell the server what kind of action the client wants to perform.\n",
    "\n",
    "**GET Request**\n",
    "\n",
    "The GET method is the most common HTTP request type. It is used to _retrieve_ information from a server. When you enter a URL into your browser, you're sending a GET request to the server asking it to send back some HTML content.\n",
    "\n",
    "In Python, we can use the `requests` library to send a HTTP GET request.\n",
    "\n",
    "As an introduction, say you are taking an organic chemistry course and there aren't enough excervises with solutions to practise on. You might find a webpagae that contains lots of examples with indivdual files, it would be tedieous to go through and download all of these manually so lets try and do it manually. We will use the Sparr Group at Basel University as an example.\n",
    "\n",
    "https://sparr.chemie.unibas.ch/en/teaching/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Making a GET request:**\n",
    "\n",
    "This is how you make a GET request to a website:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = requests.get('https://sparr.chemie.unibas.ch/en/teaching/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Checking the status code:**\n",
    "\n",
    "First we need to check if our request was successful. We can find this and other information about the status of our requestion using the 'status code'\n",
    "\n",
    "The status code tells you about the status of your HTTP request. For example, a status code of 200 means that your request was successful, a status code of 404 means the resource was not found on the server.\n",
    "\n",
    "A full list of HTTP response codes can be found here.\n",
    "\n",
    "https://en.wikipedia.org/wiki/List_of_HTTP_status_codes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(res.status_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `get` method is used to send a GET request to a server. The URL of the desired resource is passed as an argument.\n",
    "\n",
    "**Accessing the response:**\n",
    "\n",
    "When we make a request to a web server, the server responds with data and some metadata like status code, content-type, etc. This data is stored in the response. \n",
    "\n",
    "* `.text` gives you the response from the server as a string:\n",
    "* `.json()` gives you the response as a JSON object, if the response was in the format of JSON:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(res.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(res.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**HTTP headers:**\n",
    "\n",
    "HTTP headers allow the client and the server to pass additional information with the request or the response. Headers include information like content type of the response, date, status code, etc.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(res.headers)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is extremely important when writing requests into a piece of software that the code segment is included in a 'try and except' block. \n",
    "\n",
    "In the context of making HTTP requests using the `requests` library, there are several types of exceptions that can occur due to various reasons, such as:\n",
    "\n",
    "1. `requests.exceptions.Timeout`: This exception is raised when a request times out.\n",
    "\n",
    "2. `requests.exceptions.TooManyRedirects`: This error occurs if a request exceeds the configured number of maximum redirections.\n",
    "\n",
    "3. `requests.exceptions.HTTPError`: This exception is raised for certain types of invalid HTTP responses, like a 404 not found or 500 internal error.\n",
    "\n",
    "4. `requests.exceptions.RequestException`: This is a base exception from which all the above exceptions inherit, and it's raised for all other types of exceptions.\n",
    "\n",
    "When making a request to a web server, it is always possible that the server might not respond, there could be a network problem, or we may not get the expected data back. In these situations, a Python script without error handling would simply crash and stop execution. To prevent this, we use `try` and `except` blocks when making HTTP requests. By doing this, we can catch these exceptions, handle them appropriately (possibly by just printing an error message), and continue with the rest of our code instead of having our entire application crash."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from requests.exceptions import RequestException\n",
    "\n",
    "url = 'https://httpbin.org/get'\n",
    "\n",
    "try:\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()\n",
    "except RequestException as err:\n",
    "    print(f\"An Error Occured: {err}\")\n",
    "else:\n",
    "    print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above code, the `raise_for_status()` function is used to raise an `HTTPError` if an error occurs (i.e., if the HTTP request returned an unsuccessful status code). If the request is successful, it will print the response text. If an error occurs during the request, it will be handled, and the error message will be printed out without crashing the script."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have access to the web page and its contents as a python object, we need to be able to find specific elements and do things with them. For example, we might want to find all images and download them. Web pages are structured in a language called HTML, it can be tricky to read and interpret so we use a package called beautifulsoup to convert this HTML in a machine handlable code called a parse tree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we need to create a BeautifulSoup object which we do as follows. The first argument is the raw HTML content, the second argument 'html.parser' is the html parser to parse the page.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(res.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can navigate the BeautifulSoup parse tree using attributes like `.contents`, `.parent`, `.next_sibling`, `.prev_sibling`\n",
    "\n",
    "Here is an example of using `.contents` which returns a list of all children of a tag:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['How do I activate JavaScript in this web browser?']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "first_link = soup.a\n",
    "first_link_contents = first_link.contents\n",
    "print(first_link_contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can search the BeautifulSoup parse tree using methods like `.find()`, `.find_all()`, `.find_next()`, `.find_previous()`\n",
    "\n",
    "Here is an example of using `.find_all()` which returns all elements with a certain tag:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_links = soup.find_all('a')\n",
    "print(all_links)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Accessing tag attributes:**\n",
    "\n",
    "You can access the attributes of a HTML tag using indexing:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_link = soup.a\n",
    "href = first_link['href']\n",
    "print(href)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TASK **Extracting all the URLs found within a pageâ€™s `<a>` tags:**\n",
    "The attribute 'href' specifies a link to a webpage. for example the HTML ```<a href=\"https://sparr.chemie.unibas.ch/en/teaching/\">Visit Sparr Group</a>``` would display text 'Visit Sparr Group' which when clicked on would direct to the webpage. Here the 'a' represents a 'tag' which you can search for using .findall().\n",
    "\n",
    "Your task is to use the `requests` library to send a GET request to 'https://example.com'. Parse the response text with BeautifulSoup and print out all the URLs found within a page's `<a>` tags.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Your Code Here ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we want to extract specific data from a webpage, it is essential to know the nested structure that is present in the original HTML as this structure will be reflected in the BeautifulSoup object. When inspecting a webpage HTML we might find that all the files we want to download are contained with in a <file>. Then we could simply use .findall() to locate all files of interest without complex navigation through the nested structure.\n",
    "\n",
    "First, open up a webpage in Google Chrome. Let's use \"https://google.com\" for instance. \n",
    "\n",
    "Next, move your cursor to the element you are interested in. Right-click on that element, and in the dropdown menu, click on the \"Inspect\" option. This would open up the Developer Tools on the right-hand side or at the bottom of your browser depending on your settings.\n",
    "\n",
    "The Developer Tools panel comprises of many different tools, but for our current purpose, we'll focus primarily on the \"Elements\" pane. This pane show us the HTML structure of the webpage.\n",
    "\n",
    "In the Elements pane, you'll see a lot of HTML, which is the code used to create the structure and contents of the web page. The part of the page you right-clicked to inspect would be highlighted in the HTML on the panel. You can browse through the HTML to find other elements as well.\n",
    "\n",
    "If you want to find a specific HTML element in the code, hover over the different parts of the HTML code in the \"Elements\" pane, corresponding sections of the web page will be highlighted. This can help you figure out which part of the HTML corresponds to the data you're interested in.\n",
    "\n",
    "This process only covers the basic functionality needed for web scraping. Developer Tools in browsers are very robust and have many other functionalities that can be very helpful when building or debugging websites.\n",
    "\n",
    "For inspecting web pages in other browsers like Firefox, Safari, or Internet Explorer, you can use similar steps but the exact navigation and layout of developer tools might vary slightly. If you're not sure, it's best to do a quick Google search for \"How to inspect webpage in [Your Browser]\". "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quesstion 2: \n",
    "\n",
    "Use *requests* to access https://sparr.chemie.unibas.ch/en/teaching/ through a browser and inspect the page HTML. Under Group Seminar, 2024, there is a list of Excercises and Solutions. Your task is to find BeautifulSoup code which will access all of the exercises and solutions for 2024. You should end up with 10 links in total. Do this without referring to ChatGPT or a similar chatbot. You can use other resources such as the <a href=\"https://beautiful-soup-4.readthedocs.io/en/latest/\">Beautiful Soup documentation</a> or Stack Exchange. You could do this by using hard coded naviations through the internal structure, but try to find a pattern that enables .findall() to directly retrieve the files. You do not need to download the files, just verify that you have found the links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Your Code Here ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 3: \n",
    "Provide the HTML code to ChatGPT and ask it to find the links for you. Play around until you get the same answer as your manually implemented code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ChatGPT's Code Here ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 4\n",
    "\n",
    "A large number of machine learning applications use SMILES as a data representation format. This format encodes a molecule as a string. Although excellent for machine learning the SMILES format is very difficult for humans to interpret and understand. To get around this, there exist several software tools which convert SMILES strings into an image of a molecular structure.\n",
    "\n",
    "The most commonly used tool is CDK Depict. This is only avaliable as a Java package, so to access it through python code we must use the web page.\n",
    "\n",
    "https://www.simolecule.com/cdkdepict/depict.html\n",
    "\n",
    "The webpage takes a SMILES string as input and then executes some JavaScript code to generate an image. This adds a layer of complexity as JavaScript can only be executed from inside a browser. To do this from python code you must use a 'headless browser'. This is a essentially a web browser backend without the graphical user interface.\n",
    "\n",
    "We will use 'pyppeteer' as a headless browser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pyppeteer in /home/dparm/.local/lib/python3.10/site-packages (2.0.0)\n",
      "Requirement already satisfied: certifi>=2023 in /home/dparm/.local/lib/python3.10/site-packages (from pyppeteer) (2024.2.2)\n",
      "Requirement already satisfied: websockets<11.0,>=10.0 in /home/dparm/.local/lib/python3.10/site-packages (from pyppeteer) (10.4)\n",
      "Requirement already satisfied: pyee<12.0.0,>=11.0.0 in /home/dparm/.local/lib/python3.10/site-packages (from pyppeteer) (11.1.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.42.1 in /home/dparm/.local/lib/python3.10/site-packages (from pyppeteer) (4.66.1)\n",
      "Requirement already satisfied: urllib3<2.0.0,>=1.25.8 in /home/dparm/.local/lib/python3.10/site-packages (from pyppeteer) (1.26.18)\n",
      "Requirement already satisfied: importlib-metadata>=1.4 in /home/dparm/.local/lib/python3.10/site-packages (from pyppeteer) (6.8.0)\n",
      "Requirement already satisfied: appdirs<2.0.0,>=1.4.3 in /home/dparm/.local/lib/python3.10/site-packages (from pyppeteer) (1.4.4)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/lib/python3/dist-packages (from importlib-metadata>=1.4->pyppeteer) (1.0.0)\n",
      "Requirement already satisfied: typing-extensions in /home/dparm/.local/lib/python3.10/site-packages (from pyee<12.0.0,>=11.0.0->pyppeteer) (4.11.0)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: nest_asyncio in /home/dparm/.local/lib/python3.10/site-packages (1.6.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install pyppeteer\n",
    "!pip install nest_asyncio\n",
    "!pip install asyncio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" version=\"1.2\" width=\"28.4mm\" height=\"5.87mm\" viewBox=\"0 0 28.4 5.87\">\n",
       "  <desc>Generated by the Chemistry Development Kit (http://github.com/cdk)</desc>\n",
       "  <g stroke-linecap=\"round\" stroke-linejoin=\"round\" stroke=\"#000000\" stroke-width=\".3\" fill=\"#000000\">\n",
       "    <rect x=\".0\" y=\".0\" width=\"29.0\" height=\"6.0\" fill=\"rgba(0,0,0,0.00)\" stroke=\"none\"/>\n",
       "    <g id=\"mol1\" class=\"mol\">\n",
       "      <line id=\"mol1bnd1\" class=\"bond\" x1=\".76\" y1=\"1.81\" x2=\"6.48\" y2=\"5.11\"/>\n",
       "      <line id=\"mol1bnd2\" class=\"bond\" x1=\"6.48\" y1=\"5.11\" x2=\"10.62\" y2=\"2.72\"/>\n",
       "      <line id=\"mol1bnd3\" class=\"bond\" x1=\"13.79\" y1=\"2.72\" x2=\"17.92\" y2=\"5.11\"/>\n",
       "      <line id=\"mol1bnd4\" class=\"bond\" x1=\"17.92\" y1=\"5.11\" x2=\"22.21\" y2=\"2.63\"/>\n",
       "      <path id=\"mol1atm3\" class=\"atom\" d=\"M12.21 .82q-.36 .0 -.57 .26q-.21 .26 -.21 .72q.0 .45 .21 .72q.21 .26 .57 .26q.35 .0 .56 -.26q.21 -.26 .21 -.72q.0 -.45 -.21 -.72q-.21 -.26 -.56 -.26zM12.21 .56q.5 .0 .8 .34q.3 .34 .3 .91q.0 .57 -.3 .91q-.3 .34 -.8 .34q-.51 .0 -.81 -.34q-.3 -.34 -.3 -.91q.0 -.57 .3 -.91q.3 -.34 .81 -.34z\" stroke=\"none\"/>\n",
       "      <g id=\"mol1atm5\" class=\"atom\">\n",
       "        <path d=\"M22.73 .61h.43l1.06 2.01v-2.01h.32v2.4h-.44l-1.06 -2.0v2.0h-.31v-2.4z\" stroke=\"none\"/>\n",
       "        <path d=\"M24.82 .61h.32v.98h1.18v-.98h.32v2.4h-.32v-1.14h-1.18v1.14h-.32v-2.4z\" stroke=\"none\"/>\n",
       "        <path d=\"M27.16 3.57h.68v.16h-.91v-.16q.11 -.11 .3 -.31q.19 -.19 .24 -.25q.09 -.1 .13 -.18q.04 -.07 .04 -.14q.0 -.12 -.08 -.19q-.08 -.07 -.21 -.07q-.09 .0 -.19 .03q-.1 .03 -.22 .1v-.2q.12 -.05 .22 -.07q.1 -.03 .19 -.03q.23 .0 .36 .11q.13 .11 .13 .3q.0 .09 -.03 .17q-.03 .08 -.12 .19q-.02 .03 -.15 .16q-.13 .13 -.36 .37z\" stroke=\"none\"/>\n",
       "      </g>\n",
       "    </g>\n",
       "  </g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.SVG object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import asyncio\n",
    "import nest_asyncio\n",
    "import requests\n",
    "from IPython.display import SVG\n",
    "\n",
    "from pyppeteer import launch\n",
    "\n",
    "# We use this to allow nested asyncio calls. This is required for Jupyter Notebooks. \n",
    "nest_asyncio.apply()\n",
    "\n",
    "\n",
    "async def get_depiction(smiles='CCOCN'):\n",
    "    # Launch a new browser and create a new page for the browser\n",
    "    # Use the 'await' keyword so your code waits for the functions to be executed and return a value\n",
    "\n",
    "    ### Your Code Here ###\n",
    "\n",
    "    await page.goto('https://www.simolecule.com/cdkdepict/depict.html')\n",
    "\n",
    "    # Find the appropriate input field and enter the SMILES string\n",
    "    await page.evaluate(f'document.querySelector(\"#REPLACE_WITH_CORRECT_TAG\").value = \"{smiles}\"')\n",
    "\n",
    "    # Trigger the weppage, think about what key you would use to trigger an action on a webpage and use the internet to find out how to emulate that key press\n",
    "    ### Your Code Here ###\n",
    "\n",
    "\n",
    "    # Wait for JavaScript to update the image\n",
    "    await asyncio.sleep(2)\n",
    "\n",
    "    image_url = await page.querySelectorEval('#result img', '(el) => el.src')\n",
    "\n",
    "    await browser.close()\n",
    "\n",
    "    # Now you have the image url, you can use requests to get the image content\n",
    "\n",
    "    ### Your Code Here ###\n",
    "    \n",
    "    \n",
    "\n",
    "image = asyncio.get_event_loop().run_until_complete(get_depiction(smiles='CCOCN'))\n",
    "display(SVG(image))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
